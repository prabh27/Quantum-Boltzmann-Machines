{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the qRBM demo!\n",
    "In this demo we will go over a simple use case for an RBM that is trained semi-classically, semi-quantumly. To get started, we import our dependencies, we will need pyquil, grove, numpy and scipy. \n",
    "\n",
    "## Creating the underlying distribution\n",
    "We flip an unbiased coin to generate a sequence of random bits, this will be the underlying hidden distribution of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyquil.quil import Program\n",
    "import pyquil.api as api\n",
    "from pyquil.gates import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qRBM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dd7d6726b772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCNOT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqRBM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_visible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_quantum_measurements\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'qRBM' is not defined"
     ]
    }
   ],
   "source": [
    "qvm = get_qc('9q-square-qvm')\n",
    "p = Program()\n",
    "p.inst(H(0), CNOT(0, 1))\n",
    "r = qRBM(qvm, num_visible=4, num_hidden=1, n_quantum_measurements=None, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyquil.api import local_forest_runtime\n",
    "from pyquil import get_qc, Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyquil.api as api\n",
    "qvm_connection = api.QVMConnection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc = get_qc('9q-square-qvm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1,  1, -1, -1,  1, -1, -1,\n",
       "       -1, -1, -1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get our imports and setup out of the way...\n",
    "from pyquil.api import local_forest_runtime\n",
    "from pyquil import get_qc, Program\n",
    "import pyquil.api as api\n",
    "from random import *\n",
    "import numpy as np\n",
    "from qRBM_final import qRBM\n",
    "# qvm = get_qc('9q-square-qvm')\n",
    "qvm = api.QVMConnection()\n",
    "\n",
    "\n",
    "#flip a coin\n",
    "np.random.seed(1234)\n",
    "random_coin = np.random.choice([-1,1], size=20, replace=True)\n",
    "random_coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating encoded sequences\n",
    "Code below takes our sequence of random bits and encodes each 1 bit result into a 4 bit representation. This effectively creates artificial data with a 1 bit \"needle\" (hidden code subspace) in the \"haystack\" (4-bit data), which our RBM will have to decode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1, -1, -1],\n",
       "       [ 1,  1, -1, -1],\n",
       "       [-1, -1,  1,  1],\n",
       "       [ 1,  1, -1, -1],\n",
       "       [-1, -1,  1,  1],\n",
       "       [-1, -1,  1,  1],\n",
       "       [-1, -1,  1,  1],\n",
       "       [ 1,  1, -1, -1],\n",
       "       [ 1,  1, -1, -1],\n",
       "       [ 1,  1, -1, -1],\n",
       "       [ 1,  1, -1, -1],\n",
       "       [ 1,  1, -1, -1],\n",
       "       [-1, -1,  1,  1],\n",
       "       [-1, -1,  1,  1],\n",
       "       [ 1,  1, -1, -1],\n",
       "       [-1, -1,  1,  1],\n",
       "       [-1, -1,  1,  1],\n",
       "       [-1, -1,  1,  1],\n",
       "       [-1, -1,  1,  1],\n",
       "       [-1, -1,  1,  1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encode this coin flip in to an artifically high dimensional dataset\n",
    "artificial_data = []\n",
    "for flip in random_coin:\n",
    "    if flip == 1:\n",
    "        artificial_data.append([1,1,-1,-1]) #logical 1\n",
    "    else:\n",
    "        artificial_data.append([-1,-1,1,1]) #logical 0\n",
    "\n",
    "#We know have an artificially high dimensional dataset that still only has the 1 bit coin flip dictating\n",
    "#the underlying distribution\n",
    "artificial_data = np.asarray(artificial_data)\n",
    "artificial_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training our qRBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now setup our RBM to try and understand the artifical data's underlying distribution\n",
    "\n",
    "#we will use \"analytical\" measurement to save time on simulation.\n",
    "qr = qRBM(qvm, num_visible=4, num_hidden=1,n_quantum_measurements=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#let it train...\n",
    "qr.train(artificial_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing RBM-decoded data to originally encoded data\n",
    "In the following code snippet we examine the hidden unit activation probabilities and how they correspond to the initial information from the coin flip. As we can see the probability of the hiddens corresponds very well with the flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBM Pr. | Original Coin Value\n",
      "--------------------\n",
      " 0.000  |  1.000\n",
      " 0.000  |  1.000\n",
      " 1.000  | -1.000\n",
      " 0.000  |  1.000\n",
      " 1.000  | -1.000\n",
      " 1.000  | -1.000\n",
      " 1.000  | -1.000\n",
      " 0.000  |  1.000\n",
      " 0.000  |  1.000\n",
      " 0.000  |  1.000\n",
      " 0.000  |  1.000\n",
      " 0.000  |  1.000\n",
      " 1.000  | -1.000\n",
      " 1.000  | -1.000\n",
      " 0.000  |  1.000\n",
      " 1.000  | -1.000\n",
      " 1.000  | -1.000\n",
      " 1.000  | -1.000\n",
      " 1.000  | -1.000\n",
      " 1.000  | -1.000\n"
     ]
    }
   ],
   "source": [
    "#Now that the training is done (~5 mins when doing analytical expectation)\n",
    "# we can transform our data to the hidden layer\n",
    "transformed = qr.transform(artificial_data)\n",
    "\n",
    "comparison = np.stack((transformed[:,0], random_coin))\n",
    "\n",
    "#compare our rbm probabilities with the coin flips.\n",
    "print ('RBM Pr. | Original Coin Value')\n",
    "print ('-'*20)\n",
    "for i in range(len(transformed)):\n",
    "    print ('{: 0.3f}'.format(float(comparison[:,i][0])), ' |', '{: 0.3f}'.format(comparison[:,i][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you!\n",
    "Check out \n",
    "https://github.com/MichaelBroughton/QABoM.\n",
    "For a baseline version of this code that you can experiment with for yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
